import tensorflow as tf
import numpy as np

def create_dataset():
    featureset = []
    test_size = 0.2
    data_size = 100

    for i in range(data_size):
        features = np.ones((11,12,12,12), dtype=np.float32)
        classification = np.array([1.], dtype=np.float32)
        optimal_lables = np.array([1.], dtype=np.float32)
        featureset.append([features, classification, optimal_lables])
    
    testing_size = int(test_size*data_size)
    features = np.array(featureset)   
    
    train_x = list(features[:,0][:-testing_size])
    train_y = list(features[:,1][:-testing_size])
    test_x = list(features[:,0][-testing_size:])
    test_y = list(features[:,1][-testing_size:])
    train_lables = list(features[:,2][-testing_size:])
    test_labels = list(features[:,2][-testing_size:])

    #print (train_x[0], train_y[0], test_x[0], test_y[0])
    #print (len(train_x), len(train_y), len(test_x), len(test_y))
    #print (test_y)
    #print (len(train_x[0]))

    return train_x,train_y,test_x,test_y,train_lables,test_labels




train_x,train_y,test_x,test_y,train_lables,test_labels = create_dataset()

n_classes = 1
batch_size = 10

x = tf.placeholder('float')
y = tf.placeholder('float')

keep_rate = 0.8
keep_prob = tf.placeholder(tf.float32)

def conv3d(x, W):
    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')

def maxpool3d(x):
    #                        size of window         movement of window
    return tf.nn.max_pool3d(x, ksize=[1,3,3,3,1], strides=[1,2,2,2,1], padding='SAME')
    #Our output tensor produced by max_pooling2d() the 2x2 filter reduces height and width by 50% each.

    
def convolutional_neural_network(x):
    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,11,16])),
               'W_conv2':tf.Variable(tf.random_normal([3,3,3,16,32])),
               'W_conv3':tf.Variable(tf.random_normal([3,3,3,32,32])),
               'W_conv4':tf.Variable(tf.random_normal([3,3,3,32,64])),
               'W_conv5':tf.Variable(tf.random_normal([3,3,3,64,128])),
               'W_conv6':tf.Variable(tf.random_normal([3,3,3,128,128])),
               'W_conv7':tf.Variable(tf.random_normal([3,3,3,128,256])),
               'W_conv8':tf.Variable(tf.random_normal([3,3,3,256,512])),
               #fc: first element is determined by result of layers below.
               'W_fc1':tf.Variable(tf.random_normal([512,256])),
               'W_fc2':tf.Variable(tf.random_normal([256,128])),
               'out':tf.Variable(tf.random_normal([128, n_classes]))}

    biases = {'b_conv1':tf.Variable(tf.random_normal([16])),
              'b_conv2':tf.Variable(tf.random_normal([32])),
              'b_conv3':tf.Variable(tf.random_normal([32])),
              'b_conv4':tf.Variable(tf.random_normal([64])),
              'b_conv5':tf.Variable(tf.random_normal([128])),
              'b_conv6':tf.Variable(tf.random_normal([128])),
              'b_conv7':tf.Variable(tf.random_normal([256])),
              'b_conv8':tf.Variable(tf.random_normal([512])),
              'b_fc1':tf.Variable(tf.random_normal([256])),
              'b_fc2':tf.Variable(tf.random_normal([128])),
              'out':tf.Variable(tf.random_normal([n_classes]))}

    #The methods in the layers module for creating convolutional and pooling layers for two-dimensional 
    #image data expect input tensors to have a shape of [batch_size, image_height, image_width, channels] by default. 
    x = tf.reshape(x, shape=[-1, 12, 12, 12, 11])

    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])
    conv1 = maxpool3d(conv1)
    
    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])
    conv2 = tf.layers.batch_normalization(conv2)
    conv2 = maxpool3d(conv2)
    
    conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])
    conv3 = tf.layers.batch_normalization(conv3)

    conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])
    conv4 = tf.layers.batch_normalization(conv4)
    conv4 = maxpool3d(conv4)
    
    conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])
    conv5 = tf.layers.batch_normalization(conv5)
    
    conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])
    conv6 = tf.layers.batch_normalization(conv6)

    conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])
    conv7 = tf.layers.batch_normalization(conv7)
    
    conv8 = tf.nn.relu(conv3d(conv7, weights['W_conv8']) + biases['b_conv8'])
    conv8 = tf.layers.batch_normalization(conv8)
    conv8 = maxpool3d(conv8)

    fc = tf.reshape(conv8,[-1, 1*1*1*512])
    
    fc1 = tf.nn.relu(tf.matmul(fc, weights['W_fc1'])+biases['b_fc1'])
    
    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])
    
    fc2 = tf.nn.dropout(fc2, keep_rate)

    output = tf.matmul(fc2, weights['out'])+biases['out']

    return output

def train_neural_network(x):
    prediction = convolutional_neural_network(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels=y) )
    optimizer = tf.train.AdamOptimizer(learning_rate= 0.01).minimize(cost)
    
    hm_epochs = 52
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for epoch in range(hm_epochs):
                
            epoch_loss = 0
            i=0
            while i < len(train_x):
                start = i
                end = i+batch_size
                batch_x = np.array(train_x[start:end])
                batch_y = np.array(train_y[start:end])

                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,
                                                              y: batch_y})
                epoch_loss += c
                i+=batch_size
                
            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)
            
            if (epoch!=0 and epoch%10 == 0):
                saver = tf.train.Saver()
                save_route = "/MyModel/model" + epoch + ".ckpt" 
                saver.save(sess,save_route)
        
        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))

train_neural_network(x)
